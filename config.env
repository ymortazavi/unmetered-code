# ─── Vast.ai ───────────────────────────────────────────
VAST_API_KEY="your_vast_api_key_here"

# ─── HuggingFace (optional) ────────────────────────────
# Only needed for gated/private models. Leave commented out
# or set to "none" for public repos (e.g. unsloth GGUFs).
# HF_TOKEN="your_hf_token_here"

# ─── Model (HuggingFace GGUF) ─────────────────────────
# Repository and glob pattern for GGUF files to download.
# The server auto-detects split shards.
# UD-Q4_K_XL = 123 GB weights, Unsloth Dynamic 2.0 upcasting
# ~50 t/s single, ~20 t/s per slot with 4 parallel
HF_REPO="unsloth/MiniMax-M2.5-GGUF"
HF_QUANT="UD-Q4_K_XL"
HF_INCLUDE="UD-Q4_K_XL/*"
MODEL_ALIAS="minimax-m2.5"

# ─── llama-server ──────────────────────────────────────
LLAMA_PORT=8080
# 160K context (matches README; MiniMax M2.5 supports up to 197K)
CTX_SIZE=163840
GPU_LAYERS=-1
PARALLEL=4
KV_CACHE_TYPE="q4_0"
BATCH_SIZE=4096
UBATCH_SIZE=4096

# ─── Vast.ai Instance ─────────────────────────────────
IMAGE="vastai/llama-cpp:b8054-cuda-12.9"
DISK_GB=150
